# ğŸ“˜ Week 1 â€“ Final Master Notes (Statistics & Foundations of Machine Learning)

---

## ğŸŒŸ Introduction
- **Statistics** = *Art of learning from data*.
- **Scope**:
  - **Descriptive Statistics** â†’ Summarize observed data.
  - **Inferential Statistics** â†’ Generalize from sample â†’ population.
- **Flow in ML/AI**:
  Data â†’ Descriptions â†’ Probability models â†’ Inference â†’ Hypothesis testing â†’ Regression â†’ Machine Learning.
- **Applications**: Engineering, Medicine, Economics, Sports, Business, Government, AI.
- **History**: 
  - Origin in Renaissance Europe (state â†’ statistics).
  - John Graunt (1662): mortality tables.
  - Edmund Halley (1693): life insurance models.
  - Fisher, Pearson, Gosset (*t-test*): modern inference.

**Intuition ğŸ’¡:** Statistics is a **toolbox**: Descriptive = *story of what we see*; Inferential = *educated guess about what we canâ€™t see*.

---

## 1. ğŸ“Š Types of Statistics

| Aspect            | **Descriptive Statistics**                 | **Inferential Statistics**                              |
| ----------------- | ------------------------------------------ | ------------------------------------------------------- |
| **Purpose**       | Summarize and describe existing data       | Use sample to infer about population                    |
| **Methods**       | Graphs, tables, mean, median, variance     | Estimation, Hypothesis testing, Confidence intervals    |
| **Data**          | Stand-alone                                | Generalized                                             |
| **Example**       | Average rainfall in India in 2024          | Predicting voter preference from 1000 respondents       |

ğŸ‘‰ **Flow**: Collect Data â†’ Descriptive â†’ Inferential.

**Intuition ğŸ’¡:** Descriptive = **snapshot**. Inferential = **prediction beyond data**.

---

## 2. ğŸ›ï¸ Population vs Sample

- **Population**: Entire set (e.g., all IIT Bombay students).
- **Sample**: Subset chosen for study.
- **Representativeness**: Avoid bias; cover all subgroups.
- **Sampling Error**: Error due to sample â‰  population.
  $$ SE = \frac{\sigma}{\sqrt{n}} $$

**Example:** IITB height survey â†’ 800 students (~10%) enough to estimate avg height of 8000.

---

## 3. ğŸ”€ Sampling Methods

| Method             | Description                          | Example                              | Pros                  | Cons                               |
| ------------------ | ------------------------------------ | ------------------------------------ | --------------------- | ---------------------------------- |
| **Simple Random**  | Each member = equal chance           | Pick 100 students randomly           | Unbiased              | May miss subgroup balance          |
| **Systematic**     | Every k-th item                      | Every 10th product on line           | Easy to implement     | Risk of periodic bias              |
| **Stratified**     | Divide pop. into strata, sample each | IITB: 20% girls â†’ sample reflects it | Good subgroup balance | Need subgroup info                 |
| **Cluster**        | Divide into groups, choose clusters  | Pick 2 hostels & survey all          | Efficient             | Clusters may not represent whole   |
| **Sequential**     | Collect until decision rule met       | Bulb factory defect check            | Saves cost/time       | Needs stopping criteria            |

**From Reference Notes (Topic 1 & 2)**:
- **Sampling Frame**: List of all units (roll numbers, Aadhaar, addresses).
- **Sampling Unit**: Element observed (household, patient, student).
- **Survey Steps**: Define objective â†’ Identify population â†’ Sampling scheme â†’ Pilot â†’ Data collection â†’ Estimation â†’ Reportingã€40â€ sourceã€‘ã€41â€ sourceã€‘.

**Intuition ğŸ’¡:** Stratified = *fair slices from each group*. Sequential = *stop once evidence is strong enough*.

---

## 4. ğŸ“¦ Sample Size

- **Rule of thumb**: ~10% of population.
- **Trade-off**: Accuracy â†‘ vs Cost/Time â†‘.
- **Confidence Intervals**: Range where population parameter likely lies.
- **Sequential Sampling Example**:
  - Rule: Reject if >1% defective bulbs.
  - 750 defects in 1000 â†’ Reject immediately.
  - 20 defects in 20,000 â†’ Accept batch.

**Interpretation ğŸ”:** Mirrors **real-world decision-making**: stop when evidence is enough.

---

## 5. ğŸ—‚ï¸ Data Types

```
Data Types
â”‚
â”œâ”€â”€ Categorical
â”‚   â”œâ”€â”€ Nominal
â”‚   â””â”€â”€ Ordinal
â”‚
â””â”€â”€ Numerical
    â”œâ”€â”€ Interval
    â””â”€â”€ Ratio
         â”œâ”€â”€ Discrete
         â””â”€â”€ Continuous
```

### Categorical Data
- **Nominal**: Labels only, no order.
  - Examples: Gender, Hair color.
- **Ordinal**: Ordered categories.
  - Examples: Education level, Customer satisfaction.

### Numerical Data
- **Interval**: No true zero.
  - Examples: Temperature (Â°C).
- **Ratio**: True zero exists.
  - Examples: Age, Income.
- **Discrete**: Countable.
- **Continuous**: Measurable, infinite.

**From Reference Notes (Probability & Random Variables)**:
- **Qualitative vs Quantitative Variables**.
- **Skewness**: Measure of asymmetry.
- **Kurtosis**: Peakedness (flat vs sharp distributions)ã€37â€ sourceã€‘ã€38â€ sourceã€‘.

---

## 6. ğŸ“Š Descriptive Statistics in Detail

### Graphical Tools
- **Bar Graphs**: Categorical.
- **Histograms**: Continuous, grouped.
- **Frequency Polygon**: Connect midpoints.
- **Pie Chart**: Circle slices.
- **Ogive**: Cumulative.
- **Stem & Leaf**: Keep raw values.

### Numerical Measures
1. **Central Tendency**:
   - Mean = average.
   - Median = middle.
   - Mode = most frequent.
   - **Comparison Table:**

| Measure | Robustness to Outliers | When Useful |
|---------|-------------------------|-------------|
| Mean    | Sensitive               | Total/average calculations |
| Median  | Robust                  | Skewed distributions (income) |
| Mode    | Very robust             | Categorical / most frequent values |

2. **Spread**:
   - Range = Max â€“ Min.
   - Variance: $s^2 = \frac{\sum (x_i-\bar{x})^2}{n-1}$.
   - SD = âˆšvariance.
   - Chebyshevâ€™s inequality: â‰¥ (1 â€“ 1/kÂ²) within k SD.
   - Empirical Rule (normal): 68%-95%-99.7%.

**Comparison: Chebyshev vs Empirical Rule**

| Rule | Applicability | Within 1 SD | Within 2 SD | Within 3 SD |
|------|---------------|-------------|-------------|-------------|
| Chebyshev | Any data | â‰¥ 0%        | â‰¥ 75%       | â‰¥ 89%       |
| Empirical | Normal only | ~68%       | ~95%        | ~99.7%      |

3. **Percentiles & Quartiles**:
   - Q1 (25%), Median (50%), Q3 (75%).
   - Boxplot shows spread & outliers.

4. **Correlation**:
   - Scatterplots show relation.
   - Pearson r (â€“1 â‰¤ r â‰¤ 1).
   - **Interpretation ğŸ”**: r â‰  causation.

5. **Lorenz Curve & Gini Index**:
   - Measure inequality.
   - Perfect equality â†’ 45Â° line.
   - Gini = 0 (equal) â†’ 1 (max inequality).

---

## 7. âš¡ Real-World Case Studies

- **Teaching Methods**: Compare two groupsâ€™ scores â†’ infer method effectiveness.
- **IIT Bombay Height Survey**: Small sample â†’ represent large population.
- **Bulb Factory (Sequential Sampling)**: Early accept/reject â†’ saves cost.
- **Elections**: Poll design, sampling frames crucial.
- **Medical Trials**: Representativeness ensures validity.

---

## 8. ğŸ”‘ Key Insights
- Descriptive = summary, Inferential = generalization.
- Population unseen, Sample workable.
- Larger $n$ reduces sampling error, but doesnâ€™t eliminate it.
- Data type decides valid statistical test.
- Sequential sampling mirrors **real-world decision-making**.
- Skewness & Kurtosis add shape info beyond mean/variance.

---

## ğŸ“ Formula Box (Math Anchors)

**Population Mean:**
\[
\mu = \frac{1}{N}\sum_{i=1}^N X_i
\]

**Sample Mean:**
\[
\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i
\]

**Population Variance:**
\[
\sigma^2 = \frac{1}{N}\sum_{i=1}^N (X_i - \mu)^2
\]

**Sample Variance:**
\[
s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2
\]

**Standard Deviation:**
\[
s = \sqrt{s^2}
\]

**Chebyshevâ€™s Inequality:**
\[
P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}, \quad k>1
\]

**Empirical Rule (Normal data):**
- ~68% within $\mu \pm 1\sigma$
- ~95% within $\mu \pm 2\sigma$
- ~99.7% within $\mu \pm 3\sigma$

**Lorenz Curve:**
\[
L\!\left(\frac{j}{n}\right) = \frac{x_1+x_2+\dots+x_j}{x_1+x_2+\dots+x_n}
\]

**Gini Index:**
\[
G = 1 - 2B
\]

---

## 9. ğŸ—‚ï¸ Summary of Week 1
- **Statistics** = Science of data.
- Two types: **Descriptive** (summarize) & **Inferential** (conclude).
- **Population vs Sample**: Representativeness crucial.
- **Sampling methods**: Random, Stratified, Cluster, Systematic, Sequential.
- **Sample size**: ~10% rule, balance accuracy vs cost.
- **Data types**: Nominal, Ordinal, Interval, Ratio.
- **Descriptive tools**: Graphs, Mean, Median, Mode, Variance, SD, Quartiles.
- **Shape tools**: Skewness, Kurtosis.
- **Inequality tools**: Lorenz Curve, Gini Index.
- **Key cases**: Teaching methods, IITB survey, Bulb factory.

---

## ğŸ”„ Flowchart â€“ Week 1 Concepts

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Statistics   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Descriptive    â”‚                â”‚ Inferential      â”‚
â”‚ (Summarize)    â”‚                â”‚ (Generalize)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                  â”‚
 Graphs, Charts,                   Sample â†’ Population
 Mean, Median, Mode                Estimation, Hypothesis
 Variance, SD, Quartiles           Testing, Regression
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Population â†’ Sample â†’ Representativeness â†’ Sampling Error
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Sampling Methods: Random, Stratified, Cluster, Sequential, Systematic
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Data Types: Nominal, Ordinal, Interval, Ratio â†’ Discrete/Continuous
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Shape Measures: Skewness, Kurtosis
```

---

# ğŸ“˜ Week 2 â€“ Final Master Notes (Probability, Random Variables & Sampling Distributions)

---

## ğŸŒŸ Introduction
- Week 2 extends foundations into **probability, random variables, and sampling distributions**.
- This builds the bridge: **Descriptive Statistics â†’ Probability â†’ Inference**.
- Coverage:
  - Probability concepts, axioms, and rules.
  - Random variables (discrete & continuous).
  - Expectation, variance, covariance, correlation.
  - Central Limit Theorem (CLT).
  - Sampling distributions (Normal, t, Ï‡Â², F).
  - Estimation (Point, Interval, MLE, Bayesian).

**Intuition ğŸ’¡:** Week 1 told us *â€œwhat data saysâ€*. Week 2 begins to answer *â€œhow data behaves under uncertainty and repetitionâ€*.

---

## 1. ğŸ² Elements of Probabilityã€77â€ sourceã€‘

### Interpretations
- **Frequentist:** Probability = long-run relative frequency.
- **Subjective:** Probability = degree of belief.

**Key Insight:** No matter the interpretation, **the mathematical rules are identical**.

### Sample Space & Events
- **Sample Space (S):** All possible outcomes.
- **Event (E):** Subset of S.
- **Null event (âˆ…):** No outcomes.

**Examples:**
- Toss 2 dice â†’ \(S=\{(i,j) : i,j=1..6\}\).
- Event A: â€œSum=7.â€

### Event Operations
- Union: \(EâˆªF\) = either E or F occurs.
- Intersection: \(Eâˆ©F\) = both occur.
- Complement: \(E^c\) = event does not occur.
- DeMorganâ€™s Laws:
  - \((EâˆªF)^c = E^câˆ©F^c\)
  - \((Eâˆ©F)^c = E^câˆªF^c\)

### Probability Axioms
1. \(0 â‰¤ P(E) â‰¤ 1\)
2. \(P(S)=1\)
3. If E and F disjoint: \(P(EâˆªF)=P(E)+P(F)\)

**Useful Results:**
- \(P(E^c)=1-P(E)\)
- \(P(EâˆªF)=P(E)+P(F)-P(EF)\)

### Conditional Probability
\[
P(E|F)=\frac{P(EF)}{P(F)},\quad P(F)>0
\]

Multiplication Rule:
\[
P(EF)=P(F)P(E|F)
\]

### Independence
- E, F independent if \(P(EF)=P(E)P(F)\).

### Bayesâ€™ Theorem
\[
P(F_j|E)=\frac{P(E|F_j)P(F_j)}{\sum_i P(E|F_i)P(F_i)}
\]

**Intuition ğŸ’¡:** Bayes = *Update belief with evidence*. Common in **diagnostics, spam filters, reliability tests**.

---

## 2. ğŸ² Random Variablesã€76â€ sourceã€‘ã€79â€ sourceã€‘

### Definition
- **Random Variable (RV):** A function mapping outcomes â†’ numbers.

### Types
- **Discrete (DRV):** Countable outcomes. â†’ Probability Mass Function (PMF).
- **Continuous (CRV):** Interval values. â†’ Probability Density Function (PDF).

### PMF
\[
p(x)=P(X=x), \quad \sum_x p(x)=1
\]

### PDF
\[
P(a<Xâ‰¤b)=\int_a^b f(x)dx, \quad \int_{-âˆ}^{âˆ} f(x)dx=1
\]

### CDF
\[
F(x)=P(Xâ‰¤x)
\]
- Non-decreasing, limits: 0â†’1.

### Expectation & Variance
- Discrete: \(E[X]=\sum xp(x)\).
- Continuous: \(E[X]=\int xf(x)dx\).
- Variance:
\[
Var(X)=E[(X-Î¼)^2]=E[X^2]-(E[X])^2
\]

### Indicator RV

\[
I_A =
\begin{cases}
1, & \text{if $A$ occurs} \\
0, & \text{otherwise}
\end{cases}
\]

- E[I_A]=P(A).

### Joint Distributions
- Discrete: \(p(x,y)=P(X=x,Y=y)\).
- Continuous: \(f(x,y)\).
- Marginals: \(f_X(x)=âˆ«f(x,y)dy\).

### Covariance & Correlation
\[
Cov(X,Y)=E[(X-Î¼_X)(Y-Î¼_Y)]
\]
\[
Ï=\frac{Cov(X,Y)}{Ïƒ_XÏƒ_Y},\quad -1â‰¤Ïâ‰¤1
\]

### Common Distributions
- **Bernoulli:** Success/failure.
- **Binomial:** #successes in n trials.
- **Poisson:** Rare event counts.
- **Uniform:** Equal chance interval.
- **Normal:** Bell curve.
- **Exponential:** Time between events.

**Comparison: DRV vs CRV**
| Aspect | DRV | CRV |
|--------|-----|-----|
| Values | Countable | Interval |
| Example | Dice, Binomial | Height, Normal |
| Probability at point | Possible | Always 0 |
| Representation | PMF | PDF |

**Intuition ğŸ’¡:** RVs convert random outcomes into **manageable numeric models**.

---

## 3. ğŸ“ Sampling Distributionsã€78â€ sourceã€‘ã€80â€ sourceã€‘

### Sample Mean
\[
\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i
\]
- E[\bar{X}]=Î¼.
- Var(\bar{X})=ÏƒÂ²/n.

### Central Limit Theorem (CLT)
\[
\frac{\bar{X}-Î¼}{Ïƒ/âˆšn} \approx N(0,1)
\]
- Explains universality of Normal.
- Rule: nâ‰¥30 sufficient.

### Sample Variance
\[
SÂ²=\frac{âˆ‘(X_i-\bar{X})Â²}{n-1}
\]
- E[SÂ²]=ÏƒÂ² (unbiased).

### From Normal Populations
- \(\bar{X}âˆ¼N(Î¼,ÏƒÂ²/n)\).
- (nâ€“1)SÂ²/ÏƒÂ² âˆ¼ Ï‡Â²(nâ€“1).
- Independence of mean & variance â†’ **t-distribution**:
\[
T=\frac{âˆšn(\bar{X}-Î¼)}{S}âˆ¼t_{n-1}
\]

### Finite Population Case
- Hypergeometric: exact.
- Binomial approx valid if Nâ‰«n.

### Key Distribution Uses
| Distribution | Appears in | Use |
|--------------|------------|-----|
| Normal | CLT, large-sample | General inference |
| t | Mean, unknown Ïƒ, small n | CI, tests |
| Ï‡Â² | Variances | Goodness of fit |
| F | Ratio of variances | ANOVA |

**Intuition ğŸ’¡:** Sampling distributions explain **â€œwhat happens if I resample repeatedly.â€**

---

## 4. ğŸ“ Estimationã€75â€ sourceã€‘

### Point Estimation
- Example: \(\bar{X}\) estimates Î¼.
- Good estimator = **Unbiased, Consistent, Efficient, Sufficient**.

### Interval Estimation
General form:
\[
Î¸Ì‚ Â± (CriticalÃ—SE)
\]
- Mean, Ïƒ known: \(\bar{X}Â±Z_{Î±/2}Ïƒ/âˆšn\).
- Mean, Ïƒ unknown: \(\bar{X}Â±t_{Î±/2,n-1}S/âˆšn\).
- Proportion: \(pÌ‚ Â± Z_{Î±/2}âˆš(pÌ‚(1-pÌ‚)/n)\).

### Maximum Likelihood Estimation (MLE)
\[
Î¸Ì‚_{MLE}=argmax_Î¸ L(Î¸)=argmax_Î¸âˆf(x|Î¸)
\]

### Bayesian Estimation
\[
Posterior: Ï€(Î¸|x)âˆf(x|Î¸)Ï€(Î¸)
\]
- Incorporates prior belief.
- Posterior mean/Posterior mode (MAP).

**Comparison: Point vs Interval vs Bayesian**
| Aspect | Point | Interval | Bayesian |
|--------|-------|----------|----------|
| Output | Single value | Range | Posterior distribution |
| Uncertainty | Not explicit | Explicit CI | Explicit via posterior |
| Approach | Frequentist | Frequentist | Bayesian |

**Intuition ğŸ’¡:**
- Point = â€œbest guess.â€
- Interval = â€œsafe margin.â€
- Bayesian = â€œbelief update.â€

---

## ğŸ“ Formula Box

**Probability**
- \(P(E^c)=1-P(E)\)
- \(P(EâˆªF)=P(E)+P(F)-P(EF)\)
- \(P(E|F)=P(EF)/P(F)\)
- Bayes: \(P(F_j|E)=P(E|F_j)P(F_j)/âˆ‘P(E|F_i)P(F_i)\)

**Random Variables**
- E[X]=âˆ‘xp(x) or âˆ«xf(x)dx
- Var(X)=E[XÂ²]-(E[X])Â²
- Cov(X,Y)=E[XY]-E[X]E[Y]
- Corr=Cov(X,Y)/(Ïƒ_XÏƒ_Y)

**Sampling**
- Mean: \(\bar{X}=âˆ‘X_i/n\)
- Var(\bar{X})=ÏƒÂ²/n
- Variance: \(SÂ²=âˆ‘(X-\bar{X})Â²/(n-1)\)
- CLT: (\bar{X}-Î¼)/(Ïƒ/âˆšn)â†’N(0,1)
- Normal sample: (n-1)SÂ²/ÏƒÂ²âˆ¼Ï‡Â², T=âˆšn(\bar{X}-Î¼)/Sâˆ¼t

**Estimation**
- CI: Î¸Ì‚ Â± (CriticalÃ—SE)
- MLE: Î¸Ì‚=argmaxÎ¸ L(Î¸)
- Bayes: PosteriorâˆLikelihoodÃ—Prior

---

## ğŸ”„ Flowchart: Probability to Inference
```
ğŸ¯ Experiment â†’ Outcomes (Sample Space)
   â†“
ğŸ“‚ Define Events â†’ Apply Axioms
   â†“
ğŸ² Random Variables â†’ PMF/PDF/CDF
   â†“
ğŸ“ Expectation & Variance â†’ Covariance, Correlation
   â†“
ğŸ“Š Sampling â†’ Mean & Variance
   â†“
ğŸ”„ CLT â†’ Normal Approximation
   â†“
ğŸ“Š Distributions: Normal, t, Ï‡Â², F
   â†“
ğŸ“ Estimation â†’ Point, Interval, MLE, Bayesian
   â†“
âœ… Prepares for Hypothesis Testing
```

---

## ğŸ—‚ï¸ Snapshot (Quick Revision)
- Probability = rules of uncertainty.
- RV = outcomes â†’ numbers.
- DRV (PMF) vs CRV (PDF).
- Expectation=center, Variance=spread.
- CLT explains Normalâ€™s universality.
- Sampling Distributions = Normal, t, Ï‡Â², F.
- Estimation: Point, Interval, Bayesian.

---

# ğŸ“˜ Week 3 â€“ Final Master Notes (Random Variables, Sampling Distributions & Estimation)

---

## ğŸŒŸ Introduction
- Week 3 deepens probability foundations into **Random Variables, Expectations, Variance, Covariance, Sampling Distributions, and Estimation techniques**.
- We learn how **real-world outcomes â†’ numerical models (RVs) â†’ probability rules â†’ statistical inference**.
- Main coverage:
  - Random Variables: Discrete & Continuous.
  - Probability Mass Function (PMF), Probability Density Function (PDF), Cumulative Distribution Function (CDF).
  - Expectation, Variance, Covariance, Correlation.
  - Joint Distributions & Conditional Distributions.
  - Central Limit Theorem (CLT).
  - Sampling Distributions: Normal, t, Ï‡Â², F.
  - Estimation: Point, Interval, Ratio/Product Estimators.

**Intuition ğŸ’¡:** Random Variables (RVs) are the **bridge** between uncertainty and mathematics. Sampling Distributions explain **â€œwhat happens if I sample repeatedlyâ€**. Estimation tells us **â€œhow to guess population truths from data.â€**

---

## 1. ğŸ² Random Variablesã€108â€ sourceã€‘ã€110â€ sourceã€‘

### Definition
- **Random Variable (RV):** Numerical outcome of a random experiment.
- Examples:
  - Toss 2 dice â†’ RV = sum of dice.
  - Reservoir â†’ RV = water level at seasonâ€™s end.
  - Quality check â†’ RV = # acceptable components.

### Types
- **Discrete (DRV):** Countable outcomes.
  - Defined by **Probability Mass Function (PMF):**
    \[
    p(x)=P(X=x), \quad \sum_x p(x)=1
    \]
  - Example: Dice sum âˆˆ {2,â€¦,12}.

- **Continuous (CRV):** Interval values.
  - Defined by **Probability Density Function (PDF):**
    \[
    P(a<Xâ‰¤b)=\int_a^b f(x)dx, \quad \int_{-âˆ}^{âˆ} f(x)dx=1
    \]
  - Example: Lifespan of a bulb.

- **Cumulative Distribution Function (CDF):**
  \[
  F(x)=P(Xâ‰¤x)
  \]
  Properties: non-decreasing, F(â€“âˆ)=0, F(âˆ)=1.

### Indicator Variables
\[
I_A=\begin{cases}1,&A\text{ occurs}\\0,&\text{otherwise}\end{cases}
\]
- E[I_A]=P(A).

**Comparison Table: Discrete vs Continuous**
| Aspect | DRV | CRV |
|--------|-----|-----|
| Values | Countable | Interval |
| Representation | PMF | PDF |
| P(X=a) | Possible >0 | Always 0 |
| Example | Dice, Binomial | Height, Normal |

**Intuition ğŸ’¡:** DRV = *count outcomes*. CRV = *measure outcomes*.

---

## 2. ğŸ“Š Expectation & Varianceã€108â€ sourceã€‘ã€110â€ sourceã€‘

### Expectation (Mean)
- Discrete:
  \[
  E[X]=\sum x p(x)
  \]
- Continuous:
  \[
  E[X]=\int x f(x) dx
  \]
- Properties: E[aX+b]=aE[X]+b.

### Variance (Spread)
\[
Var(X)=E[(X-Î¼)^2]=E[X^2]-(E[X])^2
\]
- Standard deviation = âˆšVar(X).
- Properties:
  - Var(aX+b)=aÂ²Var(X).
  - Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y).

### Covariance
\[
Cov(X,Y)=E[(X-Î¼_X)(Y-Î¼_Y)]=E[XY]-E[X]E[Y]
\]

### Correlation
\[
Ï=\frac{Cov(X,Y)}{Ïƒ_XÏƒ_Y},\quad -1â‰¤Ïâ‰¤1
\]

**Interpretation ğŸ”:**
- E[X] = center of mass.
- Var(X) = spread around center.
- Cov/Ï = linear dependence.

---

## 3. ğŸ”— Joint & Conditional Distributionsã€108â€ sourceã€‘ã€110â€ sourceã€‘

### Joint Distributions
- Discrete: p(x,y)=P(X=x,Y=y).
- Continuous: f(x,y), with
  \[
  P((X,Y)âˆˆA)=\iint_A f(x,y)dxdy
  \]

### Marginals
- Discrete: P(X=x)=âˆ‘_y p(x,y).
- Continuous: f_X(x)=âˆ« f(x,y)dy.

### Conditional
- Discrete:
  \[
  P(X=x|Y=y)=\frac{p(x,y)}{P(Y=y)}
  \]
- Continuous:
  \[
  f_{X|Y}(x|y)=\frac{f(x,y)}{f_Y(y)}
  \]

### Independence
- X and Y independent if p(x,y)=p_X(x)p_Y(y).

**Intuition ğŸ’¡:** Joint = *whole picture*. Marginal = *one slice*. Conditional = *given info*. Independence = *no effect of one on other*.

---

## 4. ğŸ“ Sampling Distributionsã€109â€ sourceã€‘

### Sample Mean
\[
\bar{X}=\frac{1}{n}âˆ‘X_i,\quad E[\bar{X}]=Î¼,\ Var(\bar{X})=ÏƒÂ²/n
\]

### Sample Variance
\[
SÂ²=\frac{âˆ‘(X_i-\bar{X})Â²}{n-1},\quad E[SÂ²]=ÏƒÂ²
\]

### Central Limit Theorem (CLT)
\[
\frac{\bar{X}-Î¼}{Ïƒ/âˆšn}âˆ¼N(0,1)\quad (nâ‰¥30)
\]
- Explains why Normal curve appears universally.
- Binomial approx Normal if np(1â€“p)â‰¥10.

### From Normal Populations
- If X_iâˆ¼N(Î¼,ÏƒÂ²):
  - \(\bar{X}âˆ¼N(Î¼,ÏƒÂ²/n)\).
  - (nâ€“1)SÂ²/ÏƒÂ²âˆ¼Ï‡Â²(nâ€“1).
  - Independence â†’ t-distribution:
    \[
    T=\frac{âˆšn(\bar{X}-Î¼)}{S}âˆ¼t_{n-1}
    \]

### Finite Population Case
- Hypergeometric(N,Np,n).
- Approx â†’ Binomial if Nâ‰«n.

**Key Distributions Comparison**
| Distribution | When | Shape | Uses |
|--------------|------|-------|------|
| Normal | CLT, population normal | Symmetric | General inference |
| t | Mean, small n, unknown Ïƒ | Heavier tails | CI, tests |
| Ï‡Â² | Variances | Right-skewed | Tests of variance |
| F | Ratio of variances | Right-skewed | ANOVA, regression |

**Intuition ğŸ’¡:** Sampling distributions answer *â€œWhat if I resample?â€*.

---

## 5. ğŸ“ Estimationã€111â€ sourceã€‘ã€112â€ sourceã€‘

### Point Estimation
- Single value: Î¸Ì‚.
- Example: \(\bar{X}\) estimates Î¼.
- Good estimator properties: **Unbiased, Consistent, Efficient, Sufficient**.

### Interval Estimation (CI)
- General form:
  \[
  Î¸Ì‚ Â± (Critical Ã— SE)
  \]
- Mean (Ïƒ known): \(\bar{X}Â±Z_{Î±/2}Ïƒ/âˆšn\).
- Mean (Ïƒ unknown): \(\bar{X}Â±t_{Î±/2,n-1}S/âˆšn\).
- Proportion: \(pÌ‚Â±Z_{Î±/2}âˆš(pÌ‚(1-pÌ‚)/n)\).

### Ratio Estimationã€112â€ sourceã€‘
- Use auxiliary variable X (known mean).
- Estimator:
  \[
  \hat{Y}_R=\frac{\bar{y}}{\bar{x}}X
  \]
- Efficient if Ï>0.

### Product Estimationã€112â€ sourceã€‘
- For negative correlation.
- Estimator:
  \[
  \hat{Y}_P=\bar{y}\cdot\frac{\bar{X}}{\bar{x}}
  \]

**Comparison Table**
| Estimator | Works Best When | Bias | Efficiency |
|-----------|----------------|------|------------|
| Sample Mean | Baseline | Unbiased | â€“ |
| Ratio | Ï>0 strong | Slight bias | More efficient |
| Product | Ï<0 | Slight bias | More efficient |

**Intuition ğŸ’¡:** Estimation is like **guessing population truth**: point = single guess, interval = safety margin, ratio/product = smarter guess with extra info.

---

## ğŸ“ Formula Box

**Random Variables**
- PMF: \(p(x)=P(X=x)\).
- PDF: \(P(a<Xâ‰¤b)=âˆ«_a^b f(x)dx\).
- CDF: \(F(x)=P(Xâ‰¤x)\).
- E[X]=âˆ‘xp(x) (discrete), âˆ«xf(x)dx (continuous).
- Var(X)=E[XÂ²]-(E[X])Â².
- Cov(X,Y)=E[XY]-E[X]E[Y].
- Corr=Cov(X,Y)/(Ïƒ_XÏƒ_Y).

**Sampling**
- Mean: \(\bar{X}=âˆ‘X_i/n\).
- Var(\bar{X})=ÏƒÂ²/n.
- Variance: \(SÂ²=âˆ‘(X-\bar{X})Â²/(n-1)\).
- CLT: (\bar{X}-Î¼)/(Ïƒ/âˆšn)â†’N(0,1).
- (n-1)SÂ²/ÏƒÂ²âˆ¼Ï‡Â², T=âˆšn(\bar{X}-Î¼)/Sâˆ¼t.

**Estimation**
- CI: Î¸Ì‚ Â± (CriticalÃ—SE).
- Ratio: \(\hat{Y}_R=(\bar{y}/\bar{x})X\).
- Product: \(\hat{Y}_P=\bar{y}(\bar{X}/\bar{x})\).

---

## ğŸ”„ Flowchart â€“ From RVs to Estimation
```
Experiment â†’ Random Variable (X)
   â†“
PMF/PDF â†’ Expectation, Variance
   â†“
Joint Distributions â†’ Covariance, Correlation
   â†“
Samples â†’ Sample Mean & Variance
   â†“
CLT â†’ Normal Approximation
   â†“
Sampling Distributions â†’ Normal, t, Ï‡Â², F
   â†“
Estimation â†’ Point, Interval, Ratio, Product
```

---

## ğŸ—‚ï¸ Snapshot (Quick Revision)
- RV = outcome â†’ number.
- DRV (PMF), CRV (PDF).
- Expectation=center, Variance=spread.
- Covariance & Correlation â†’ dependence.
- CLT explains Normalâ€™s universality.
- Sampling Distributions: Normal, t, Ï‡Â², F.
- Estimation: Point, Interval, Ratio, Product.

---

# ğŸ“˜ Week 5 â€“ Final Master Notes (Probability, Conditional Probability, Bayesâ€™ Theorem & Independence)

---

## ğŸŒŸ Introduction
- Week 5 emphasizes **probability theory as the foundation for inferential statistics and machine learning**.
- Main concepts:
  - Sample spaces & events.
  - Probability axioms & basic propositions.
  - Counting principles (permutations & combinations).
  - Conditional probability & Law of Total Probability.
  - Bayesâ€™ theorem.
  - Independence of events.
- Reinforced via TA problems: dice, cards, medical tests, birthday paradox, and reliability systems.

**Intuition ğŸ’¡:** Probability = *consistent rules for uncertainty*. It enables reasoning from partial knowledge to conclusions.

---

## 1. ğŸ² Sample Space & Events

### Definitions
- **Sample Space (S):** All possible outcomes.
- **Event (E):** Subset of S.
- **Null Event (âˆ…):** Impossible outcome.
- **Certain Event (S):** Always occurs.

**Examples:**
- Toss a coin â†’ S={H,T}.
- Roll a die â†’ S={1,2,3,4,5,6}.
- Gender â†’ S={Boy,Girl}.

### Event Operations
- Union: \(EâˆªF\) â†’ either E or F.
- Intersection: \(Eâˆ©F\) â†’ both occur.
- Complement: \(E^c\) â†’ E does not occur.
- Subset: \(EâŠ‚F\).

**DeMorganâ€™s Laws:**
- \((EâˆªF)^c=E^câˆ©F^c\).
- \((Eâˆ©F)^c=E^câˆªF^c\).

---

## 2. ğŸ“ Axioms of Probability (Kolmogorov)
1. \(0â‰¤P(E)â‰¤1\).
2. \(P(S)=1\).
3. If \(E_1,E_2,â€¦\) are mutually exclusive:
\[
P\Big(\bigcup_i E_i\Big)=\sum_i P(E_i).
\]

### Propositions
- \(P(E^c)=1âˆ’P(E)\).
- \(P(EâˆªF)=P(E)+P(F)âˆ’P(Eâˆ©F)\).
- If \(EâŠ‚F\), then P(E)â‰¤P(F).

**Example (Smoking in Nevada):**
- 28% men smoke cigarettes, 6% cigars, 3% both.
- P(smoke either) = 0.28+0.06âˆ’0.03=0.31.
- P(neither)=0.69.

### Odds
\[
\text{Odds}(A)=\frac{P(A)}{1-P(A)}
\]
- Example: If P(A)=3/4 â†’ Odds=3:1.

---

## 3. âš–ï¸ Equally Likely Outcomes (Classical)
\[
P(E)=\frac{\#\text{favourable outcomes}}{\#\text{total outcomes}}
\]

**Examples:**
- Die roll â†’ P(even)=3/6=0.5.
- Bowl: 6 white, 5 black â†’ choose 2 without replacement. Total=\(\binom{11}{2}=55\). Favourable=30. Probability=30/55=6/11.
- Birthday paradox: With 23 people, probability â‰¥2 share a birthday >0.5.

---

## 4. ğŸ§® Counting Principles
- **Multiplication Principle:** m outcomes Ã— n outcomes â†’ mÂ·n total.
- **Permutations:**
\[
^nP_r=\frac{n!}{(n-r)!}
\]
- **Combinations:**
\[
^nC_r=\binom{n}{r}=\frac{n!}{(n-r)!r!}
\]

**Examples:** Books arrangement, committees, roommates pairing.

---

## 5. ğŸ¯ Conditional Probability
\[
P(E|F)=\frac{P(Eâˆ©F)}{P(F)},\quad P(F)>0.
\]

**Examples:**
- Dice: Given first die=3, P(sum=8)=1/6.
- Transistors: Given not defective, P(acceptable)=25/35=5/7.
- Father-son dinner: P(both boys | at least one boy)=1/3.

**Intuition ğŸ’¡:** Conditional = *updating probability when information is known*.

---

## 6. ğŸ“Š Law of Total Probability
If {Fâ‚,â€¦,Fâ‚™} partition S:
\[
P(E)=\sum_{i=1}^n P(E|F_i)P(F_i).
\]

**Example (Insurance):**
- Accident prone (30%): P(accident)=0.4.
- Not accident prone (70%): P(accident)=0.2.
- Overall: 0.4Â·0.3+0.2Â·0.7=0.26.

---

## 7. ğŸ§¾ Bayesâ€™ Theorem
\[
P(F_j|E)=\frac{P(E|F_j)P(F_j)}{\sum_i P(E|F_i)P(F_i)}.
\]

### Examples
- **Medical Test:** Sensitivity=0.99, Specificity=0.99, Prevalence=0.005.
\[
P(D|+) = \frac{0.99Â·0.005}{0.99Â·0.005+0.01Â·0.995}=0.332.
\]
- **Twins problem:** P(identical|same sex)â‰ˆ0.28.
- **Criminal case:** Prior guilt=0.6, given rare evidence â†’ Posteriorâ‰ˆ0.88.

**Intuition ğŸ’¡:** Bayes = *prior belief updated by evidence*.

---

## 8. ğŸ”— Independence
\[
P(Eâˆ©F)=P(E)P(F).
\]

**Properties:**
- Symmetric.
- E â«« F â‡’ E â«« Fá¶œ.

**Examples:**
- Cards: A=Ace, H=Heart â†’ Independent.
- Dice: Sum=7 independent of first die=4.
- Reliability (parallel system):
\[
P(\text{system works})=1-\prod_i (1-p_i).
\]

---

## 9. ğŸ“˜ Random Variables (Lecture6 content)
- **DRV:** PMF p(x)=P(X=x).
- **CRV:** PDF f(x), with âˆ« f(x)dx=1.
- **CDF:** F(x)=P(Xâ‰¤x).
- **Indicator RV:** I_A=1 if A occurs, else 0.
- **Joint Distributions:** F(x,y)=P(Xâ‰¤x,Yâ‰¤y). Independence if f(x,y)=f_X(x)f_Y(y).

---

## 10. ğŸ“˜ TA Problems & Solutions
- Dice, card, committees, birthday paradox.
- Medical test Bayes example.
- Reliability parallel system formula.
- Explicit combinatorics (\(\binom{n}{r}\)) shown.

---

## ğŸ“ Formula Box
- Conditional: \(P(E|F)=P(Eâˆ©F)/P(F)\).
- Total Probability: \(P(E)=\sum P(E|F_i)P(F_i)\).
- Bayes: \(P(F_j|E)=\frac{P(E|F_j)P(F_j)}{\sum P(E|F_i)P(F_i)}\).
- Reliability (parallel): \(P(\text{system})=1-\prod (1-p_i)\).

---

## ğŸ“Š Comparison Table: Conditional vs Independence
| Aspect | Conditional | Independent |
|--------|-------------|-------------|
| Formula | P(E|F)=P(Eâˆ©F)/P(F) | P(Eâˆ©F)=P(E)P(F) |
| Info needed | Given F | None |
| Interpretation | Updated prob with info | No effect between events |

---

## ğŸ”„ Flowchart â€“ Probability Concepts
```
Sample Space â†’ Events â†’ Set Operations
   â†“
Axioms of Probability
   â†“
Equally Likely Outcomes â†’ Counting Principles
   â†“
Conditional Probability
   â†“
Law of Total Probability
   â†“
Bayesâ€™ Theorem
   â†“
Independence
   â†“
Applications: Dice, Cards, Medical Tests, Reliability
```

---

## ğŸ—‚ï¸ Snapshot (Quick Revision)
- Sample space, events, axioms.
- Equally likely â†’ ratio rule.
- Counting â†’ permutations, combinations.
- Conditional â†’ info-based update.
- Total Probability â†’ weighted sum.
- Bayes â†’ belief update.
- Independence â†’ no effect.
- Random Variables: PMF, PDF, CDF, Joint.

---
